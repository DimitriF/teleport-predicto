{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicto: Dataset creation and model training\n",
    "\n",
    "Notebook to create a dataset in order to train a lanquage model able to autocomplete tsx lines of code or css rules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## install cssbeautify-cli \n",
    "# ! npm install -g cssbeautify-cli\n",
    "\n",
    "# !pip install transformers\n",
    "# !pip install tokenizers\n",
    "# !pip install PyGithub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.decoders import ByteLevel as ByteLevelDecoder\n",
    "from tokenizers.normalizers import NFKC, Sequence\n",
    "from tokenizers.pre_tokenizers import ByteLevel\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from tokenizers import ByteLevelBPETokenizer\n",
    "\n",
    "from transformers import GPT2Config, GPT2LMHeadModel, GPT2Tokenizer# loading tokenizer from the saved model path\n",
    "\n",
    "import gc\n",
    "import os\n",
    "import shutil\n",
    "import re\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "import requests\n",
    "\n",
    "import getpass\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from github import Github\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## We need a list with all css properties\n",
    "\n",
    "response = requests.get(\"https://gist.githubusercontent.com/davidhund/3bd6757d6a36a283b0a2933666bd1976/raw/4ccc47ed835c1be6bf0bfe00a32f427874da917c/all-css-properties.json\")\n",
    "data = response.json()\n",
    "data = [x['property'] for x in data[2:]]\n",
    "all_css_properties = []\n",
    "for i in data:\n",
    "    if not i in all_css_properties:\n",
    "        all_css_properties.append(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## https://github.com/topics\n",
    "# topic = 'css'\n",
    "topic = 'typescript'\n",
    "\n",
    "if topic == 'css':\n",
    "    filetype = 'css'\n",
    "elif topic == 'typescript':\n",
    "    filetype = 'tsx'\n",
    "\n",
    "\n",
    "\n",
    "EXPERIMENT = 'predicto_' + topic\n",
    "experiment_path = \"data/\" + EXPERIMENT + \"/\"\n",
    "\n",
    "path_test = 'data/predicto_' + topic + '_test.txt'\n",
    "path_train = 'data/predicto_' + topic + '_train.txt'\n",
    "\n",
    "\n",
    "if not os.path.exists(experiment_path):\n",
    "    os.mkdir(experiment_path)\n",
    "\n",
    "n_repo = 100 ## max 1000 repo to take from github\n",
    "\n",
    "vocab_size=5000\n",
    "n_positions=64\n",
    "\n",
    "n_layer=8\n",
    "n_head=8\n",
    "n_emb=128\n",
    "batch_size = 64\n",
    "num_train_epochs = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_comments(text):\n",
    "    \"\"\" remove c-style comments. From https://www.saltycrane.com/blog/2007/11/remove-c-comments-python/\n",
    "        text: blob of text with comments (can include newlines)\n",
    "        returns: text with comments removed\n",
    "    \"\"\"\n",
    "    pattern = r\"\"\"\n",
    "                            ##  --------- COMMENT ---------\n",
    "           /\\*              ##  Start of /* ... */ comment\n",
    "           [^*]*\\*+         ##  Non-* followed by 1-or-more *'s\n",
    "           (                ##\n",
    "             [^/*][^*]*\\*+  ##\n",
    "           )*               ##  0-or-more things which don't start with /\n",
    "                            ##    but do end with '*'\n",
    "           /                ##  End of /* ... */ comment\n",
    "         |                  ##  -OR-  various things which aren't comments:\n",
    "           (                ## \n",
    "                            ##  ------ \" ... \" STRING ------\n",
    "             \"              ##  Start of \" ... \" string\n",
    "             (              ##\n",
    "               \\\\.          ##  Escaped char\n",
    "             |              ##  -OR-\n",
    "               [^\"\\\\]       ##  Non \"\\ characters\n",
    "             )*             ##\n",
    "             \"              ##  End of \" ... \" string\n",
    "           |                ##  -OR-\n",
    "                            ##\n",
    "                            ##  ------ ' ... ' STRING ------\n",
    "             '              ##  Start of ' ... ' string\n",
    "             (              ##\n",
    "               \\\\.          ##  Escaped char\n",
    "             |              ##  -OR-\n",
    "               [^'\\\\]       ##  Non '\\ characters\n",
    "             )*             ##\n",
    "             '              ##  End of ' ... ' string\n",
    "           |                ##  -OR-\n",
    "                            ##\n",
    "                            ##  ------ ANYTHING ELSE -------\n",
    "             .              ##  Anything other char\n",
    "             [^/\"'\\\\]*      ##  Chars which doesn't start a comment, string\n",
    "           )                ##    or escape\n",
    "    \"\"\"\n",
    "    regex = re.compile(pattern, re.VERBOSE|re.MULTILINE|re.DOTALL)\n",
    "    noncomments = [m.group(2) for m in regex.finditer(text) if m.group(2)]\n",
    "\n",
    "    return \"\".join(noncomments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get a few github repo based on the selected topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " ········································\n"
     ]
    }
   ],
   "source": [
    "## input here your github api key: https://github.com/settings/tokens\n",
    "github_api_key = getpass.getpass() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = Github(github_api_key)\n",
    "\n",
    "repos = g.search_repositories(query=f'topic:{topic}',sort='stars')\n",
    "\n",
    "it = 0\n",
    "repos_clean = []\n",
    "for repo in repos[:n_repo]:\n",
    "    it += 1\n",
    "    repos_clean.append(repo.full_name)\n",
    "    if it % 50 == 0:\n",
    "        time.sleep(1) ## we have to sleep a bit to give the api some rest\n",
    "\n",
    "random.shuffle(repos_clean)\n",
    "repos_clean_test = repos_clean[:int(len(repos_clean)/10)]\n",
    "repos_clean_train = repos_clean[int(len(repos_clean)/10):]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean and extract the files of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_line_number: 62168 for data/predicto_typescript_test.txt\n",
      "total_file_number: 531 for data/predicto_typescript_test.txt\n",
      "total_line_number: 454185 for data/predicto_typescript_train.txt\n",
      "total_file_number: 5443 for data/predicto_typescript_train.txt\n"
     ]
    }
   ],
   "source": [
    "for repos_clean, path_data in zip([repos_clean_test, repos_clean_train],\n",
    "                                                     [path_test, path_train]):\n",
    "\n",
    "#     file_out = open(path_data, \"w\",)\n",
    "    output_lines = []\n",
    "\n",
    "    total_line_number = 0\n",
    "    total_file_number = 0\n",
    "    for repo in repos_clean:\n",
    "        if repo == 'cdnjs/cdnjs': ## need to remove this one...\n",
    "            continue\n",
    "            \n",
    "        folder = re.sub('/','-',repo)\n",
    "        os.system('git clone  --depth 1 https://github.com/' + repo + ' data/' + folder)\n",
    "        paths = []\n",
    "        paths += [str(x) for x in Path(\"data/\"+folder+'/').glob(\"**/*.\" + filetype)]\n",
    "        line_number = 0\n",
    "        for path in paths:\n",
    "            path = path.replace(\" \", \"\\\\ \")\n",
    "            if os.path.isfile(path):\n",
    "                ## Remove comment\n",
    "                with open(path, 'r', encoding=\"utf8\", errors='ignore') as f:\n",
    "                    code_w_comments = f.read()\n",
    "                code_wo_comments = remove_comments(code_w_comments)\n",
    "                fh = open(path, \"w\")\n",
    "                fh.write(code_wo_comments)\n",
    "                fh.close()\n",
    "                if topic == 'css':\n",
    "                    if os.path.exists('/tools/node/lib/node_modules/cssbeautify-cli/bin/cssbeautify-cli'):\n",
    "                        os.system('/tools/node/lib/node_modules/cssbeautify-cli/bin/cssbeautify-cli -f '+\n",
    "                                  path +' > ' + path)\n",
    "                    else:\n",
    "                        os.system(''~/node_modules/.bin/cssbeautify-cli -f '+\n",
    "                                  path +' > ' + path)\n",
    "                ## if the final file exist, append all lines to the list\n",
    "                if os.path.isfile(path):\n",
    "                    file_in = open(path, \"r\", errors='ignore')\n",
    "                    lines = file_in.readlines()\n",
    "                    for line in lines:\n",
    "                        if (line != '\\n'):                 \n",
    "                            output_lines.append(line)\n",
    "                            line_number += 1\n",
    "                    file_in.close()      \n",
    "        shutil.rmtree(\"data/\"+folder)\n",
    "        total_line_number +=  line_number\n",
    "        total_file_number +=  len(paths)\n",
    "    print('total_line_number: ' + str(total_line_number) + ' for ' + path_data)\n",
    "    print('total_file_number: ' + str(total_file_number) + ' for ' + path_data)\n",
    "    \n",
    "    if topic == 'css':\n",
    "        ## some more cleaning to get one css rules per line\n",
    "        file_out = open(path_data, \"w\",)\n",
    "        l = ''\n",
    "        for line in output_lines:\n",
    "            if (line != '}\\n'):  \n",
    "                line = re.sub('\\n','',line)\n",
    "                line = re.sub(' +',' ',line)\n",
    "                l += line\n",
    "            else:\n",
    "                l += '}\\n'\n",
    "                if not l.startswith('@'):\n",
    "                    file_out.write(l)\n",
    "                l = ''\n",
    "        file_out.close()\n",
    "    else:\n",
    "        file_out = open(path_data, \"w\",)\n",
    "        for line in output_lines:\n",
    "            file_out.write(line)\n",
    "        file_out.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizer training\n",
    "\n",
    "We need to remove long sample to avoid infering unfinished samples at test time, for this we first train a tokenizer and then applying it on our datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data/predicto_typescript/vocab.json', 'data/predicto_typescript/merges.txt']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tokenizers import ByteLevelBPETokenizer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "\n",
    "special_tokens=[\"<s>\",\"<pad>\",\"</s>\",\"<unk>\",\"<mask>\",]\n",
    "if topic == 'css':\n",
    "    special_tokens += all_css_properties\n",
    "\n",
    "tokenizer = ByteLevelBPETokenizer()\n",
    "tokenizer.pre_tokenizer = Whitespace()\n",
    "tokenizer.train(files=[path_train],\n",
    "                vocab_size=vocab_size,\n",
    "                min_frequency=2,\n",
    "                special_tokens=[\n",
    "                    \"<s>\",\n",
    "                    \"<pad>\",\n",
    "                    \"</s>\",\n",
    "                    \"<unk>\",\n",
    "                    \"<mask>\",]\n",
    "               ),\n",
    "tokenizer.save_model(experiment_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "453752 originally lines in data/predicto_typescript_train.txt\n",
      "62167 originally lines in data/predicto_typescript_test.txt\n"
     ]
    }
   ],
   "source": [
    "for path in [path_train, path_test]:\n",
    "    file_in = open(path,'r',)\n",
    "    lines = file_in.readlines()    \n",
    "    file_in.close()\n",
    "    print(str(len(lines)) + ' originally lines in ' + path)\n",
    "    file_out = open(path, \"w\",)\n",
    "    for l in lines:\n",
    "        if len(tokenizer.encode(l)) <= n_positions:\n",
    "            file_out.write(l)\n",
    "    file_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "453420 after subset lines in data/predicto_typescript_train.txt\n",
      "61976 after subset lines in data/predicto_typescript_test.txt\n"
     ]
    }
   ],
   "source": [
    "for path in [path_train, path_test]:\n",
    "    file_in = open(path,'r',)\n",
    "    lines = file_in.readlines()\n",
    "    print(str(len(lines)) + ' after subset lines in ' + path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60593664"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(experiment_path)\n",
    "\n",
    "tokenizer.add_special_tokens({\n",
    "  \"eos_token\": \"</s>\",\n",
    "  \"bos_token\": \"<s>\",\n",
    "  \"unk_token\": \"<unk>\",\n",
    "  \"pad_token\": \"<pad>\",\n",
    "  \"mask_token\": \"<mask>\"\n",
    "})# creating the configurations from which the model can be made\n",
    "config = GPT2Config(\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    bos_token_id=tokenizer.bos_token_id,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    n_layer=n_layer,\n",
    "    n_head=n_head,\n",
    "    n_emb=n_emb,\n",
    "    n_positions=n_positions\n",
    ")# creating the model\n",
    "model = GPT2LMHeadModel(config)\n",
    "model.num_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loader "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda/anaconda3/envs/pytorch/lib/python3.8/site-packages/transformers-4.1.1-py3.8.egg/transformers/data/datasets/language_modeling.py:124: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the 🤗 Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/master/examples/language-modeling/run_mlm.py\n"
     ]
    }
   ],
   "source": [
    "from transformers import LineByLineTextDataset\n",
    "\n",
    "\n",
    "dataset_test = LineByLineTextDataset(\n",
    "    tokenizer=tokenizer,\n",
    "    file_path=path_test,\n",
    "    block_size=n_positions,\n",
    ")\n",
    "\n",
    "dataset_train = LineByLineTextDataset(\n",
    "            tokenizer=tokenizer,\n",
    "            file_path=path_train,\n",
    "            block_size=n_positions,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, mlm=True, mlm_probability=0.15\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='6948' max='6948' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6948/6948 13:00, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.771101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.550462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>1.460529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>1.417736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>1.372226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>1.325910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>1.311637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>1.291306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>1.275830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>1.243019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>1.245983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>1.251930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>1.239790</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='8444' max='8444' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [8444/8444 00:48]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 2.3431520462036133, 'epoch': 1.0}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "            output_dir=experiment_path,\n",
    "            overwrite_output_dir=True,\n",
    "            num_train_epochs=num_train_epochs,\n",
    "            per_device_train_batch_size=batch_size,\n",
    "            save_steps=10_000,\n",
    "            save_total_limit=2,\n",
    "        )\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=dataset_train,\n",
    "    eval_dataset=dataset_test,\n",
    "    # prediction_loss_only=True,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save and reload the model and tokenizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(experiment_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "import re\n",
    "import torch\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(experiment_path)\n",
    "model = GPT2LMHeadModel.from_pretrained(experiment_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[94mPrediction 0\u001b[0m\n",
      "\u001b[94mimport\u001b[0m { IContext } from \"./blueprintjs/core';\n",
      "\u001b[94mPrediction 1\u001b[0m\n",
      "\u001b[94mimport\u001b[0m _Action-ui/icons/styles/';\n",
      "\u001b[94mPrediction 2\u001b[0m\n",
      "\u001b[94mimport\u001b[0m { Button } from '../../sharedUiComponents/common/styles';\n",
      "\u001b[94mPrediction 3\u001b[0m\n",
      "\u001b[94mimport\u001b[0m * as React from'react';\n",
      "\u001b[94mPrediction 4\u001b[0m\n",
      "\u001b[94mimport\u001b[0m _onsIcon from '@material-ui/core/svg';\n",
      "\u001b[94mPrediction 5\u001b[0m\n",
      "\u001b[94mimport\u001b[0m { ables, DD_MM_PTSTP_ERSER_MM_PPS_SERPI_SAOTSSER_OO__A_PO_OTOPSIMAER_CO_\n",
      "\u001b[94mPrediction 6\u001b[0m\n",
      "\u001b[94mimport\u001b[0m { useComponent, FanalIcon } from './common'\n",
      "\u001b[94mPrediction 7\u001b[0m\n",
      "\u001b[94mimport\u001b[0m {\n",
      "\u001b[94mPrediction 8\u001b[0m\n",
      "\u001b[94mimport\u001b[0m { useInput } from'material-ui/core/styles';\n",
      "\u001b[94mPrediction 9\u001b[0m\n",
      "\u001b[94mimport\u001b[0m React from \"react\";\n"
     ]
    }
   ],
   "source": [
    "num_return_sequences = 10\n",
    "\n",
    "# text = 'span {'# encoding the input text\n",
    "text = 'import'# encoding the input text\n",
    "\n",
    "input_ids = tokenizer.encode(text, return_tensors='pt')# getting out output\n",
    "beam_output = model.generate(\n",
    "    input_ids,\n",
    "    do_sample=True, \n",
    "    max_length=n_positions, \n",
    "    top_k=10, \n",
    "    top_p=0.95, temperature=1.0,\n",
    "    num_return_sequences=num_return_sequences,\n",
    "    eos_token_id=1#tokenizer.get_vocab()['}']\n",
    ")\n",
    "\n",
    "for i  in range(num_return_sequences):\n",
    "    print('\\033[94m' + 'Prediction ' + str(i) + '\\033[0m')\n",
    "    s  = tokenizer.decode(beam_output[i])\n",
    "    s = re.sub(r'<pad>', '', s)\n",
    "    s = re.sub(text,'',s)\n",
    "    print('\\033[94m' + text + '\\033[0m', end =\"\")\n",
    "    print(s)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
